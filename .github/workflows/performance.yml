name: Performance Monitoring

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'segnomms/algorithms/**'
      - 'segnomms/core/**'
      - 'tests/perf/**'
      - 'pyproject.toml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'segnomms/algorithms/**'
      - 'segnomms/core/**'
      - 'tests/perf/**'
      - 'pyproject.toml'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - quick
        - regression
        - scaling

env:
  PYTHON_VERSION: "3.11"

jobs:
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v5
      with:
        fetch-depth: 0  # Full history for baseline comparison

    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true
        cache-dependency-glob: "**/uv.lock"

    - name: Set up Python
      run: uv python install ${{ env.PYTHON_VERSION }}

    - name: Cache UV and Virtual Environment
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/uv
          .venv
        key: uv-${{ runner.os }}-python${{ env.PYTHON_VERSION }}-${{ hashFiles('**/uv.lock', 'pyproject.toml') }}
        restore-keys: |
          uv-${{ runner.os }}-python${{ env.PYTHON_VERSION }}-
          uv-${{ runner.os }}-

    - name: Install dependencies
      run: |
        uv sync
        uv pip install -e .

    - name: Cache performance baselines
      uses: actions/cache@v4
      with:
        path: |
          tests/perf/performance_baselines.json
          tests/perf/performance_metrics.json
        key: performance-baselines-${{ runner.os }}-${{ hashFiles('tests/perf/**/*.py') }}
        restore-keys: |
          performance-baselines-${{ runner.os }}-

    - name: Run performance benchmarks
      run: |
        case "${{ github.event.inputs.benchmark_type }}" in
          "quick")
            make benchmark-quick
            ;;
          "regression")
            make benchmark-regression
            ;;
          "scaling")
            make benchmark-scaling
            ;;
          *)
            make benchmark
            ;;
        esac

    - name: Generate performance report
      run: make benchmark-report

    - name: Upload performance artifacts
      uses: actions/upload-artifact@v4
      with:
        name: performance-results
        path: |
          tests/perf/performance_report.txt
          tests/perf/performance_metrics.json
          tests/perf/performance_baselines.json
          tests/perf/benchmark_results.json

    - name: Comment performance results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v8
      with:
        script: |
          const fs = require('fs');
          try {
            const reportPath = 'tests/perf/performance_report.txt';
            if (fs.existsSync(reportPath)) {
              const report = fs.readFileSync(reportPath, 'utf8');
              const truncatedReport = report.length > 60000 ?
                report.substring(0, 57000) + '\n...\n[Report truncated due to length]' :
                report;

              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## üöÄ Performance Benchmark Results\n\n\`\`\`\n${truncatedReport}\n\`\`\`\n\nüìä Full performance artifacts are available in the workflow run.`
              });
            }
          } catch (error) {
            console.log('Could not post performance comment:', error);
          }

  regression-detection:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    needs: performance-benchmarks
    if: always() && (github.event_name == 'pull_request' || github.event_name == 'push')

    steps:
    - uses: actions/checkout@v5

    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true
        cache-dependency-glob: "**/uv.lock"

    - name: Set up Python
      run: uv python install ${{ env.PYTHON_VERSION }}

    - name: Cache UV and Virtual Environment
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/uv
          .venv
        key: uv-${{ runner.os }}-python${{ env.PYTHON_VERSION }}-${{ hashFiles('**/uv.lock', 'pyproject.toml') }}
        restore-keys: |
          uv-${{ runner.os }}-python${{ env.PYTHON_VERSION }}-
          uv-${{ runner.os }}-

    - name: Install dependencies
      run: |
        uv sync
        uv pip install -e .

    - name: Download performance artifacts
      uses: actions/download-artifact@v5
      with:
        name: performance-results
        path: tests/perf/

    - name: Check for performance regressions
      run: |
        python -c "
        import json
        import sys
        from pathlib import Path

        metrics_file = Path('tests/perf/performance_metrics.json')
        if not metrics_file.exists():
            print('‚ö†Ô∏è No performance metrics found')
            sys.exit(0)

        with open(metrics_file) as f:
            metrics = json.load(f)

        # Simple regression check: look for operations taking >2x baseline
        regressions = []
        for metric in metrics[-20:]:  # Check recent metrics
            if metric.get('execution_time', 0) > 1.0:  # >1 second operations
                regressions.append(f\"{metric['name']}: {metric['execution_time']*1000:.1f}ms\")

        if regressions:
            print('‚ö†Ô∏è Potential performance regressions detected:')
            for regression in regressions:
                print(f'  - {regression}')
            # Don't fail CI on regressions, just warn
        else:
            print('‚úÖ No significant performance regressions detected')
        "

    - name: Alert on critical regressions
      if: github.event_name == 'pull_request'
      run: |
        echo "Performance regression detection completed. Check workflow logs for details."

  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - uses: actions/checkout@v5

    - name: Install uv
      uses: astral-sh/setup-uv@v7
      with:
        enable-cache: true
        cache-dependency-glob: "**/uv.lock"

    - name: Set up Python
      run: uv python install ${{ env.PYTHON_VERSION }}

    - name: Cache UV and Virtual Environment
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/uv
          .venv
        key: uv-${{ runner.os }}-python${{ env.PYTHON_VERSION }}-${{ hashFiles('**/uv.lock', 'pyproject.toml') }}
        restore-keys: |
          uv-${{ runner.os }}-python${{ env.PYTHON_VERSION }}-
          uv-${{ runner.os }}-

    - name: Install dependencies
      run: |
        uv sync
        uv pip install -e .

    - name: Install memory profiling tools
      run: |
        uv pip install psutil memory-profiler

    - name: Run memory profiling tests
      run: |
        uv run pytest tests/perf/test_memory_profiling.py -v --tb=short

    - name: Upload memory profiling results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: memory-profiling-results
        path: |
          tests/perf/memory_*.json
          tests/perf/memory_report.txt

